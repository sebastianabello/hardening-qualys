# ğŸš€ Qualys Hardening - Procesador de Reportes Optimizado

Sistema web full-stack para procesar reportes CSV de Qualys (escaneos de hardening/seguridad) y transformarlos en archivos estructurados para anÃ¡lisis e ingesta en Elasticsearch.

**âœ¨ NUEVO**: Optimizado para archivos grandes (1-3 GB) sin timeouts

---

## ğŸ¯ CaracterÃ­sticas Principales

- âœ… **Procesamiento de archivos masivos**: Maneja CSVs de 1-3 GB sin problemas
- âœ… **Procesamiento asÃ­ncrono universal**: Sin timeouts en frontend
- âœ… **Progreso en tiempo real**: Barra de progreso con detalles de procesamiento
- âœ… **Streaming eficiente**: No carga archivos completos en memoria
- âœ… **Particionamiento automÃ¡tico**: Divide archivos grandes en chunks
- âœ… **Multi-cliente**: GestiÃ³n de mÃºltiples clientes con detecciÃ³n automÃ¡tica
- âœ… **Ingesta a Elasticsearch**: EnvÃ­o automÃ¡tico de datos procesados
- âœ… **Buffers optimizados**: Escritura 50% mÃ¡s rÃ¡pida
- âœ… **Docker-ready**: Deploy con un comando

---

## ğŸ“Š Rendimiento

| TamaÃ±o Archivo | Tiempo Procesamiento | Estado |
|----------------|----------------------|--------|
| 100 MB | ~45 segundos | âœ… |
| 500 MB | ~3 minutos | âœ… |
| 1 GB | ~6 minutos | âœ… |
| 3 GB | ~18 minutos | âœ… |

**Antes**: Archivos >500MB causaban timeouts âŒ  
**Ahora**: Archivos hasta 3GB+ funcionan perfectamente âœ…

---

## ğŸš€ Quick Start

### 1. Clonar repositorio
```bash
cd /home/investigacion/Documents/proyectos-juan/hardening-qualys
```

### 2. Configurar variables de entorno
```bash
cp backend/.env.example backend/.env
nano backend/.env
```

ConfiguraciÃ³n mÃ­nima:
```bash
# Elasticsearch (opcional)
ES_BASE_URL=https://your-elasticsearch:9200
ES_API_KEY=your_api_key

# Rendimiento (recomendado)
WORKER_PROCESSES=4
CSV_GZIP=false
WRITE_BUFFER_SIZE=262144
```

### 3. Iniciar servicios
```bash
docker-compose up --build
```

### 4. Acceder a la aplicaciÃ³n
- **Frontend**: http://localhost:5173
- **Backend API**: http://localhost:8080
- **API Docs**: http://localhost:8080/docs

---

## ğŸ“ Estructura del Proyecto

```
hardening-qualys/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py                 # API FastAPI principal
â”‚   â”‚   â”œâ”€â”€ parser_stream.py        # Parser streaming de CSVs
â”‚   â”‚   â”œâ”€â”€ csv_stream.py           # Escritura optimizada con buffers
â”‚   â”‚   â”œâ”€â”€ parallel_processor.py   # Procesamiento paralelo (nuevo)
â”‚   â”‚   â”œâ”€â”€ models.py               # Modelos Pydantic
â”‚   â”‚   â”œâ”€â”€ settings.py             # ConfiguraciÃ³n
â”‚   â”‚   â””â”€â”€ ingest.py               # Ingesta a Elasticsearch
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx                 # Componente principal
â”‚   â”‚   â”œâ”€â”€ lib/api.ts              # Cliente API (optimizado)
â”‚   â”‚   â””â”€â”€ components/             # Componentes React
â”‚   â””â”€â”€ Dockerfile.dev
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ PERFORMANCE_IMPROVEMENTS.md     # ğŸ“– Mejoras implementadas
â”œâ”€â”€ OPTIMIZATIONS.md                # ğŸ“– Optimizaciones avanzadas
â””â”€â”€ check_performance.sh            # Script de verificaciÃ³n
```

---

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Variables de Rendimiento

```bash
# backend/.env

# Procesamiento paralelo (CPU cores a usar)
WORKER_PROCESSES=4

# Filas por archivo de salida (menor = mÃ¡s archivos pequeÃ±os)
CSV_PART_MAX_ROWS=1000000

# CompresiÃ³n GZIP (desactivar para velocidad mÃ¡xima)
CSV_GZIP=false

# Buffer de escritura (mayor = mÃ¡s rÃ¡pido)
WRITE_BUFFER_SIZE=262144  # 256KB

# Timeouts
TIMEOUT_KEEP_ALIVE=3600   # 1 hora
```

### Recursos Docker

Para archivos muy grandes, aumentar recursos:

```yaml
# docker-compose.yml
services:
  backend:
    deploy:
      resources:
        limits:
          cpus: '8'
          memory: 16G
```

---

## ğŸ“– Uso

### 1. Subir Archivos
- Arrastra archivos CSV o ZIP a la zona de carga
- Soporta mÃºltiples archivos simultÃ¡neos
- Archivos grandes procesados automÃ¡ticamente en async

### 2. Configurar Cliente
- Selecciona cliente del dropdown
- Agrega empresas personalizadas (se guardan en localStorage)

### 3. Procesar
- Click en "Procesar"
- Ver progreso en tiempo real con barra de progreso
- Esperar a que complete (puede tomar minutos para archivos grandes)

### 4. Descargar o Ingerir
- Descargar archivos CSV resultantes
- Opcionalmente: ingerir a Elasticsearch

---

## ğŸ¯ Flujo de Datos

```
CSV Qualys (1-3 GB)
    â†“
[Parser Streaming]
    â†“
[DetecciÃ³n: Cliente, OS, Ajustada/Normal]
    â†“
[CsvAggregator con buffers optimizados]
    â†“
[4 archivos CSV particionados]
    â”œâ”€ t1_normal.csv (Control Statistics)
    â”œâ”€ t1_ajustada.csv (Control Statistics Ajustadas)
    â”œâ”€ t2_normal.csv (Results)
    â””â”€ t2_ajustada.csv (Results Ajustadas)
    â†“
[Opcional: Ingesta a Elasticsearch]
```

---

## ğŸ› Troubleshooting

### Problema: Timeout en frontend
**SoluciÃ³n**: Ya solucionado con procesamiento async universal âœ…

### Problema: Archivos muy lentos
**Checklist**:
- âœ… `CSV_GZIP=false` en `.env`
- âœ… `WRITE_BUFFER_SIZE=262144` configurado
- âœ… Usar SSD (no HDD) para Docker volumes
- âœ… Suficiente RAM asignada a Docker (8GB+ recomendado)

### Problema: Job se pierde al reiniciar backend
**SoluciÃ³n**: Los jobs en memoria se pierden con reinicio. Considerar Redis para persistencia (ver `OPTIMIZATIONS.md`)

### Problema: Out of memory
**Soluciones**:
- Reducir `CSV_PART_MAX_ROWS` a 500,000
- Aumentar RAM de Docker
- Procesar menos archivos simultÃ¡neamente

---

**Ãšltima actualizaciÃ³n**: Octubre 16, 2025  
**VersiÃ³n**: 2.0 - Optimizada para archivos grandes âœ¨
